---
fontsize: 8pt
bibliography: 1_Referenzen.bib
citation_package: natbib
output:
  beamer_presentation:
    keep_tex: yes
    includes:
      in_header: 1_header.tex
---


```{r, include = F}
source("1_R_common.R")
```

#  {.plain}
\center
```{r, echo = FALSE, out.width = "20%"}
knitr::include_graphics("1_Abbildungen/wtfi_otto.png")
```

\vspace{2mm}

\Huge
Tutorium Allgemeines Lineares Modell
\vspace{2mm}


\normalsize
BSc Psychologie SoSe 2022

\vspace{2mm}
\text{3. Termin: (1) Regression (Teil 2)}

\vspace{18mm}
Belinda Fleischmann

\vspace{3mm}
\scriptsize
Inhalte basieren auf [ALM Kursmaterialien](https://www.ipsy.ovgu.de/Institut/Abteilungen+des+Institutes/Methodenlehre+I+_+Experimentelle+und+Neurowissenschaftliche+Psychologie/Lehre/Sommersemester+2022/Allgemeines+Lineares+Modell.html) von [Dirk Ostwald](https://www.ipsy.ovgu.de/Institut/Abteilungen+des+Institutes/Methodenlehre+I+_+Experimentelle+und+Neurowissenschaftliche+Psychologie/Team/Dirk+Ostwald.html), lizenziert unter [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/deed.de)





# Follow-up letzte Woche

\large
Wie sieht ein Summe aus Quadratfunktionen aus?

\vspace{6mm}
Zur Erinnerung: 

\footnotesize
\begin{definition}[Ausgleichsgerade]
\justifying
Für $\beta := (\beta_0,\beta_1)^T \in \mathbb{R}^2$ heißt die linear-affine Funktion
\begin{equation}
f_\beta : \mathbb{R} \to \mathbb{R}, x \mapsto f_\beta(x) := \beta_0 + \beta_1 x,
\end{equation}
für die für eine Wertemenge  $\{(x_1,y_1),...,(x_n,y_n)\} \subset \mathbb{R}^2$ die Funktion
\begin{equation}
q : \mathbb{R}^2 \to \mathbb{R}_{\ge 0}, \beta \mapsto q(\beta)
:= \sum_{i=1}^n (y_i-f_\beta(x_i))^2
 = \sum_{i=1}^n (y_i- (\beta_0 + \beta_1x_i))^2
\end{equation}
der quadrierten vertikalen Abweichungen der $y_i$ von den Funktionswerten $f_{\beta}(x_i)$
ihr Minimum annimt, die \textit{Ausgleichsgerade für die Wertemenge $\{(x_1,y_1),...,(x_n,y_n)\}$}.
\end{definition}


# Selbstkontrollfragen - Regression
\footnotesize

\setstretch{1.5}

8. Erläutern Sie die Motivation des einfachen linearen Regressionsmodells in Bezug auf die Ausgleichsgerade.
9. Definieren Sie das Generative Modell der einfachen linearen Regression.
10. Geben Sie das Theorem zum Normalverteilungsmodell der einfachen linearen Regression wieder.
11. Skizzieren das Modell der einfachen linearen Regression per Hand.
12. Skizzieren Sie eine Realisierung des Modells der einfachen linearen Regression per Hand.
13. Geben Sie das Theorem zur ML-Schätzung der Parameter der einfachen linearen Regression an.
14. Skizzieren Sie den Beweis des Theorems zur ML-Schätzung der Parameter der einfachen linearen Regression.
15. Bestimmen Sie mithilfe eines R Skripts die ML Parameterschätzer des einfachen linearen Regressionsmodells für den bereitgestellten Beispieldatensatz und visualisieren die entsprechende Regressionsgerade wie für die Ausgleichsgerade gezeigt. Geben Sie weiterhin die Bedeutung der geschätzten Parameterwerte $\hat{\beta}_0$ und $\hat{\beta}_0$  an.



# Regression - Selbstkontrollfragen
\vspace{3mm}
\setstretch{1}
\large
\color{darkblue} 8. Erläutern Sie die Motivation des einfachen linearen Regressionsmodells in Bezug auf die Ausgleichsgerade.

\vspace{9mm}
\color{black}
\small

Eine Ausgleichsgerade erlaubt Aussagen über unbeobachtete y Werte für x Werte. Der Wert von $q(\hat{\beta})$ quantifiziert die Güte der Ausgleichsgeradenpassung. Eine Ausgleichsgerade erlaubt allerdings nur implizite Aussagen über die mit der Anpassung verbundene Unsicherheit.

In der einfachen linearen Regression wird die Idee einer Ausgleichsgerade um eine probabilistische Komponente (normalverteilte Fehlervariable) erweitert, um quantitative Aussagen über die mit einer Ausgleichsgeradenanpassung verbundene Unsicherheit machen zu können. 

Weiterhin erlaubt die einfache lineare Regression, einen Hypothesentest-basierten Zugang zur Einschätzung der angepassten
Parameterwerte $\hat{\beta}_0$ und $\hat{\beta}_1$ sowie das Bestimmen von Konfidenzintervallen, die eine quantitative Aussage über die mit dem Schätzwert assoziierte Unsicherheit ermöglichen. 



# Regression - Selbstkontrollfragen
\vspace{3mm}
\setstretch{1}
\large
\color{darkblue} 9. Definieren Sie das Generative Modell der einfachen linearen Regression.

\vspace{6mm}

\color{black}
\small

\begin{definition}[Generatives Modell der einfachen linearen Regression]
Für $i = 1,...,n$ sei
\begin{equation}\label{eq:modell_generativ}
Y_i = \beta_0 + \beta_1x_i + \varepsilon_{i}
\end{equation}
wobei
\begin{itemize}
\item $x_i \in \mathbb{R}$ fest vorgegebene sogenannte \textit{Prädiktorwerte} oder \textit{Regressorwerte} sind,
\item $\beta_0,\beta_1 \in \mathbb{R}$ wahre, aber unbekannte, Parameterwerte sind und
\item $\varepsilon_{i} \sim N(0,\sigma^2)$ unabhängige und identisch normalverteilte nicht-beobachtbare Zufallsvariablen mit wahrem, aber unbekanntem, Parameter $\sigma^2 > 0$ sind.
\end{itemize}
Dann heißt \eqref{eq:modell_generativ} \textit{Generatives Modell der einfachen linearen Regression}.
\end{definition}




# Regression - Selbstkontrollfragen
\vspace{3mm}
\setstretch{1}
\large
\color{darkblue} 10. Geben Sie das Theorem zum Normalverteilungsmodell der einfachen linearen Regression wieder.
\vspace{6mm}

\color{black}
\footnotesize
\begin{theorem}[Normalverteilungsmodell der einfachen linearen Regression]
\normalfont
\justifying
Das generative Modell der einfachen linearen Regression
\begin{equation}\label{eq:modell}
Y_i = \beta_0 + \beta_1x_i + \varepsilon_{i} \mbox{ mit } \varepsilon_i \sim N(0,\sigma^2) \mbox{ u.i.v. für } i = 1,...,n
\end{equation}
lässt sich äquivalent in der Form
\begin{equation}\label{eq:modell_normal}
Y_i \sim N\left(\beta_0 + \beta_1x_i, \sigma^2\right) \mbox{ u. für } i = 1,...,n
\end{equation}
schreiben.
\end{theorem}



# Regression - Selbstkontrollfragen
\vspace{3mm}
\setstretch{1}
\large
\color{darkblue} 11. Skizzieren das Modell der einfachen linearen Regression per Hand.

\color{black}
\normalsize
\vspace{3mm}
Modell der einfachen linearen Regression

\setstretch{1.2}

```{r, echo = F, eval = F}
# Deterministischer Modellteil
n           = 10                              # Anzahl Regressor/Datenwerte
x           = 1:n                             # Regressorwerte
beta_0      = 0                               # Offsetparameter
beta_1      = 1                               # Slopeparameter
sigsqr      = 1                               # Varianzparameter
mu          = beta_0 + beta_1*x               # Normalverteilungserwartungswertparameter

# Visualisierung
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "m",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
xpd         = TRUE,
font.main   = 1,
cex         = 1,
cex.main    = 1.2)

# Deterministischer Modellteil
xlimits     = c(0,11)
ylimits     = c(-3,13)
plot(
x,
mu,
type        = "b",
lty         = 2,
pch         = 16,
xlab        = "x",
ylab        = "y",
xlim        = xlimits,
ylim        = ylimits)

# Probabilistischer Modellteil
w           = 3                          # WDF Support Width
res         = 1e3                        # WDF resolution
for(i in 1:length(x)){
  y    = seq(mu[i]-w,mu[i]+w, len = res)
  pdfy = -dnorm(y, mu[i], sigsqr)
  lines(-pdfy+mu[i],y      , col = "gray80")
  lines(-rep(0,res)+mu[i],y, col = "gray40")
}

# Regressorwerte
points(
x,
rep(ylimits[1], n),
col = "blue",
pch = 16)


# Speichern
dev.copy2pdf(
file        = file.path("1_Abbildungen", "alm_1_elr_1.pdf"),
width       = 6,
height      = 4)

```


```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics("1_Abbildungen/alm_1_elr_1.pdf")
```
\vspace{-4mm}
\footnotesize
\center

\textcolor{blue}{$\bullet$} $x_i$
\hspace{2mm}
$\bullet$ $\beta_0 + \beta_1x_i$ \mbox{ für } $\beta_0 := 0$, $\beta_1 := 1$
\hspace{2mm}
\textcolor{gray}{\textbf{---}} $N(y_i; \beta_0 + \beta_1x_i, \sigma^2)$ \mbox{ für } $\sigma^2 := 1$.






# Regression - Selbstkontrollfragen
\vspace{3mm}
\setstretch{1}
\large
\color{darkblue} 12. Skizzieren Sie eine Realisierung des Modells der einfachen linearen Regression per Hand.

\color{black}
\normalsize
\vspace{3mm}
Realisierung des Modells der einfachen linearen Regression

\setstretch{1.2}

```{r, echo = F, eval = F}
# Deterministischer Modellteil
n           = 10                              # Anzahl Regressor/Datenwerte
x           = 1:n                             # Regressorwerte
beta_0      = 0                               # Offsetparameter
beta_1      = 1                               # Slopeparameter
sigsqr      = 1                               # Varianzparameter
mu          = beta_0 + beta_1*x               # Normalverteilungserwartungswertparameter

# Datenrealisierung
y           = rep(NaN,n)  # Datenarrayinitialisierung
for(i in 1:n){
  y[i] = rnorm(1,mu[i],sigsqr)  # Y_i \sim N(\beta_0 + \beta_1x_i, \sigma^2)
}


# Visualisierung
graphics.off()
dev.new()
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "m",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
xpd         = TRUE,
font.main   = 1,
cex         = 1,
cex.main    = 1.2)

# Deterministischer Modellteil
xlimits     = c(0,11)
ylimits     = c(-3,13)
plot(
x,
mu,
type        = "b",
lty         = 2,
pch         = 16,
xlab        = "x",
ylab        = "y",
xlim        = xlimits,
ylim        = ylimits)

# Probabilistischer Modellteil
w           = 3                          # WDF Support Width
res         = 1e3                        # WDF resolution
for(i in 1:length(x)){
  yp   = seq(mu[i]-w,mu[i]+w, len = res)
  pdfy = -dnorm(yp, mu[i], sigsqr)
  lines(-pdfy+mu[i],yp      , col = "gray80")
  lines(-rep(0,res)+mu[i],yp, col = "gray40")
}

# Regressorwerte
points(
x,
rep(ylimits[1], n),
col = "blue",
pch = 16)

# Datenwerte
points(
x,
y,
col = "red",
pch = 16)


# Speichern
dev.copy2pdf(
file        = file.path("1_Abbildungen", "alm_1_elr_2.pdf"),
width       = 6,
height      = 4)

```


```{r, echo = FALSE, out.width = "80%"}
knitr::include_graphics("1_Abbildungen/alm_1_elr_2.pdf")
```
\vspace{-4mm}
\footnotesize
\center

\textcolor{blue}{$\bullet$} $x_i$
\hspace{2mm}
$\bullet$ $\beta_0 + \beta_1x_i$ \mbox{ für } $\beta_0 := 0$, $\beta_1 := 1$
\hspace{2mm}
\textcolor{gray}{\textbf{---}} $N(y_i; \beta_0 + \beta_1x_i, \sigma^2)$ \mbox{ für } $\sigma^2 := 1$
\hspace{2mm}
\textcolor{red}{$\bullet$} $(x_i,y_i)$





# Regression - Selbstkontrollfragen
\vspace{3mm}
\setstretch{1}
\large
\color{darkblue} 13. Geben Sie das Theorem zur ML-Schätzung der Parameter der einfachen linearen Regression an.
\vspace{3mm}

\color{black}
\footnotesize

\begin{theorem}[Maximum Likelihood Schätzung]
\justifying
\normalfont
Es sei
\begin{equation}\label{eq:modell}
Y_i = \beta_0 + \beta_1x_i + \varepsilon_{i} \mbox{ mit } \varepsilon_i \sim N(0,\sigma^2) \mbox{ u.i.v. für } i = 1,...,n
\end{equation}
das Modell der einfachen linearen Regression. Dann sind Maximum Likelihood
Schätzer der Modellparameter $\beta_0,\beta_1$ und $\sigma^2$ gegeben durch
\begin{equation}
\hat{\beta}_1  := \frac{c_{xy}}{s_x^2}, \,\,\,
\hat{\beta}_0  := \bar{y} - \hat{\beta}_1\bar{x} \,\,\,
\mbox{ und }
\hat{\sigma}^2 := \frac{1}{n}\sum_{i=1}^n \left(y_i - \left(\hat{\beta}_0 + \hat{\beta}_1 x_i\right)\right)^2.
\end{equation}
\end{theorem}




# Regression - Selbstkontrollfragen
\vspace{3mm}
\setstretch{1}
\large
\color{darkblue} 14. Skizzieren Sie den Beweis des Theorems zur ML-Schätzung der Parameter der einfachen linearen Regression.

\vspace{3mm}

\color{black}

\small
Teil 1/2: $\hat{\beta}_0$ und $\hat{\beta}_1$
\footnotesize

Wir wollen zunächst zeigen, dass die Ausgleichsgerardenparameter $\hat{\beta}_0$ und
$\hat{\beta}_1$ den entsprechenden ML Schätzern gleichen. 

Um die ML Schätzer zu bestimmen, formulieren wir zunächst die Likelihood-Funktion des Modells der einfachen linearen Regression in Abhängigkeit von $\beta_0$ und $\beta_1$. 

Die Likelihood-Funktion ist definiert als der Wert der gemeinsamen Verteilung der $Y_1, ...,Y_n$ in Abhängigkeit von den Parametern $\beta_0$ und $\beta_1$. 

Aufgrund der Unabhängigkeit der $Y_1, ...,Y_n$ können wir die gemeinsame Verteilung als Produkt der einzelnen Wahrscheinlichkeitsdichtefunktionen, also als Produkt von Dichtefunktionen der univariaten Normalverteilung aufschreiben. 

Die funktionale Form der Dichtefunktionen der univariaten Normalverteilung enthält eine Exponentialfunktion. Mit den Eigenschaften einer Exponentialfunktion können wir dieses Produkt umschreiben zu einer Exponentialfunktion von einem Term, der im Wesentlichen aus der negativen Summe der quadrierten Abweichungen (i.e. der Funktion $q$) besteht.

Weil für eine Exponentialfunktion gilt, dass für $a < b \le 0$ gilt, dass $\exp(a)<\exp(b)$,
wird der Exponentialterm der Likelihood-Funktion maximal, wenn $q$ minimal und entsprechend $-q$ maximal wird. 

Wie im Beweis der Ausgleichsgeradenform gezeigt, wissen wir, dass $q$ für $\hat{\beta}_0$ und $\hat{\beta}_1$, wie sie auch im Theorem zur ML-Schätzung der Parameter der einfachen linearen Rregression angegeben sind, minimal wird, und damit $\hat{\beta}_1$ und $\hat{\beta}_0$ die Likelihood-Funktion
maximieren.




# Regression - Selbstkontrollfragen
\vspace{3mm}
\setstretch{1}
\large
\color{darkblue} 14. Skizzieren Sie den Beweis des Theorems zur ML-Schätzung der Parameter der einfachen linearen Regression.

\vspace{3mm}

\color{black}
\small
Teil 2/2: $\hat{\sigma^2}$

\footnotesize
Als nächstes wollen wir zeigen, dass $\hat{\sigma^2}$ dem ML-Schätzer entspricht. 

Dazu betrachten wir analog zu oben die Likelihood-Funktion des Modells der einfachen linearen Regression, jedoch als Funktion von $\sigma^2$ und formulieren die entsprechende log-Likelihood-Funktion. 

Wir wollen das $\hat{\sigma^2}$ bestimmen, für das die (log-)Likelihood-Funktion maximial wird. 

Um die log-Likelihood-Funktion zu maximieren, bilden wir die 1. Ableitung, setzen diese gleich $0$ und lösen nach $\sigma^2$ auf. Durch umstellen erhalten wir dann die Formel zur Schätzung von $\sigma^2$, also $\hat{\sigma^2}$, wie sie im Theorem angeben ist. 

\vspace{12mm}


# Regression - Selbstkontrollfragen
\vspace{3mm}
\setstretch{1}
\large
\color{darkblue} 15. Bestimmen Sie mithilfe eines R Skripts die ML Parameterschätzer des einfachen linearen Regressionsmodells für den bereitgestellten Beispieldatensatz und visualisieren die entsprechende Regressionsgerade wie für die Ausgleichsgerade gezeigt. Geben Sie weiterhin die Bedeutung der geschätzten Parameterwerte $\hat{\beta}_0$ und $\hat{\beta}_0$  an.

\color{black}
\footnotesize




# Regression - Selbstkontrollfragen
\vspace{3mm}
\setstretch{1}
\large
\color{darkblue} 15. Bestimmen Sie mithilfe eines R Skripts die ML Parameterschätzer ...

\vspace{3mm}
\color{black}
\normalsize
Lösungsmöglichkeit A) mithilfe der Formeln für Stichprobenstatistik und ML Schätzer

\footnotesize
(vollständiges Skript in der Datei Lösung_Aufg_15.R)
\vspace{2mm}
\setstretch{1}
\tiny
```{r}
# Einlesen des Beispieldatensatzes
fname       = file.path(getwd(), "1_Daten", "1_Regression.csv")
D           = read.table(fname, sep = ",", header = TRUE)

# Stichprobenstatistiken
n           = length(D$y_i)                                      # Anzahl Datenpunkte
x_bar       = mean(D$x_i)                                        # Stichprobenmittel der x_i-Werte
y_bar       = mean(D$y_i)                                        # Stichprobenmittel der y_i-Werte
s2x         = var(D$x_i)                                         # Stichprobenvarianz der  x_i-Werte
cxy         = cov(D$x_i, D$y_i)                                  # Stichprobenkovarianz der (x_i,y_i)-Werte

# Parameteterschätzer (nach dem Theorem der ML Schätzung)
beta_1_hat  = cxy/s2x                                            # \hat{\beta}_1, Steigungsparameter
beta_0_hat  = y_bar - beta_1_hat*x_bar                           # \hat{\beta}_0, Offset Parameter
sigsqr_hat  = (1/n)*sum((D$y_i-(beta_0_hat+beta_1_hat*D$x_i))^2) # Varianzparameter

# Ausgabe
cat("beta_0_hat:"  , beta_0_hat,
    "\nbeta_1_hat:", beta_1_hat,
    "\nsigsqr_hat:", sqrt(sigsqr_hat))
```




# Regression - Selbstkontrollfragen
\vspace{3mm}
\setstretch{1}
\large
\color{darkblue} 15. Bestimmen Sie mithilfe eines R Skripts die ML Parameterschätzer ...

\vspace{3mm}
\color{black}
\normalsize
Lösungsmöglichkeit B) mithilfe der Funktion ```lm()``` der R package (```car```)

\footnotesize
(vollständiges Skript in der Datei Lösung_Aufg_15.R)

\vspace{2mm}
\setstretch{1.2}

\tiny
```{r}
# Laden des 'car' package
library(car)

# Einlesen des Beispieldatensatzes
fname       = file.path(getwd(), "1_Daten", "1_Regression.csv")
D           = read.table(fname, sep = ",", header = TRUE)

# Analyse mit lm()
model       = lm(formula = D$y_i ~ D$x_i, data = D)
print(model)
```




# Regression - Selbstkontrollfragen
\vspace{3mm}
\setstretch{1}
\large
\color{darkblue} 15. Bestimmen Sie mithilfe eines R Skripts die ML Parameterschätzer ...

\vspace{1mm}
\color{black}
\normalsize
R Code zur Visualisierung

\footnotesize
(vollständiges Skript in der Datei Lösung_Aufg_15.R)
\vspace{2mm}

\setstretch{1.2}
\tiny
```{r, echo = F, eval = F}
# Einlesen des Beispieldatensatzes
fname       = file.path(getwd(), "1_Daten", "1_Regression.csv")
D           = read.table(fname, sep = ",", header = TRUE)

# Stichprobenstatistiken
x_bar       = mean(D$x_i)               # Stichprobenmittel der x_i-Werte
y_bar       = mean(D$y_i)               # Stichprobenmittel der y_i-Werte
s2x         = var(D$x_i)                # Stichprobenvarianz der  x_i-Werte
cxy         = cov(D$x_i, D$y_i)         # Stichprobenkovarianz der (x_i,y_i)-Werte

# Ausgleichsgeradenparameter
beta_1_hat  = cxy/s2x                   # \hat{\beta}_1, Steigungsparameter
beta_0_hat  = y_bar - beta_1_hat*x_bar  # \hat{\beta}_0, Offset Parameter

# Ausgabe
cat("beta_0_hat:", beta_0_hat,
    "\nbeta_1_hat:", beta_1_hat)

graphics.off()
dev.new()
par(
  family      = "sans",
  mfcol       = c(1,1),
  pty         = "s",
  bty         = "l",
  lwd         = 1,
  las         = 1,
  mgp         = c(2,1,0),
  xaxs        = "i",
  yaxs        = "i",
  font.main   = 1,
  cex         = 1,
  cex.main    = 1.2)

# Datenwerte
plot(
  D$x_i,
  D$y_i,
  pch         = 16,
  xlab        = "Anzahl Therapiestunden (x)",
  ylab        = "Symptomreduktion (y)",
  xlim        = c(0,21),
  ylim        = c(-10, 40),
  main        = TeX("$\\hat{\\beta}_0 =  -6.19, \\hat{\\beta}_1 = 1.66$"))

# Ausgleichsgerade
abline(
  coef        = c(beta_0_hat, beta_1_hat),
  lty         = 1,
  col         = "black")
  
# Legende
legend(
"topleft",
c(TeX("$(x_i,y_i)$"), TeX("$f(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1x$")),
lty = c(0,1),
pch = c(16, NA),
bty = "n")

# Speichern
dev.copy2pdf(
file        = file.path("1_Abbildungen", "alm_1_ausgleichsgerade_3.pdf"),
width       = 4,
height      = 4)
```

\tiny
```{r, eval = F}
# Datenwerte
plot(
  D$x_i,
  D$y_i,
  pch         = 16,
  xlab        = "Anzahl Therapiestunden (x)",
  ylab        = "Symptomreduktion (y)",
  xlim        = c(0,21),
  ylim        = c(-10, 40),
  main        = TeX("$\\hat{\\beta}_0 =  -6.19, \\hat{\\beta}_1 = 1.66$"))

# Ausgleichsgerade
abline(
  coef        = c(beta_0_hat, beta_1_hat),
  lty         = 1,  # linetype (0: Punkt, 1:Linie, 2:gestrichelte Linie, usw.)
  col         = "black")

# Legende
legend(
  "topleft",
  c(TeX("$(x_i,y_i)$"), TeX("$f(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1x$")),
  lty        = c(0,1),   
  pch        = c(16, NA),  # plot character (0: Vierreck, 1:Kringel, ..., 16: ausgefüllter Kreis, usw.)
  bty        = "n")  # boxtype ("o": komplette box, "n": keine box,... usw.)
```



# Regression - Selbstkontrollfragen
\vspace{3mm}
\setstretch{1}
\large
\color{darkblue} 15. Bestimmen Sie mithilfe eines R Skripts die ML Parameterschätzer ...

\vspace{3mm}
\color{black}
\normalsize
Visualisierung

\vspace{6mm}

```{r, echo = FALSE, out.width = "55%"}
knitr::include_graphics("1_Abbildungen/alm_1_ausgleichsgerade_3.pdf")
```
